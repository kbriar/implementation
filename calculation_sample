def generate_plan(self, state: AgentState, metrics: List[str]) -> List[Dict]:
    """Generate execution plan based on intents found in the tagged data."""
    # 1. Get feedback insights
    insights = self.query_agent.feedback_collector.learn_from_feedback()
    
    # 2. Extract unique intents from state.data['intent'] with optimization
    intents = []
    if state.data is not None and 'intent' in state.data.columns:
        try:
            from itertools import chain
            intents = list(set(chain.from_iterable(state.data['intent'])))
            intents = [intent for intent in intents if intent]  # Filter out None/empty
        except Exception as e:
            logging.error(f"Intent extraction from state.data failed: {str(e)}")
            intents = []
    if not intents:
        logging.warning("No intents found in state.data. Falling back to default 'fetch_metric'.")
        intents = ["fetch_metric"]
    print(f"Derived Intents: {intents}")  # Debug intents

    # 3. Generate LLM prompt with strict structure enforcement
    prompt_template = """
    Given:
    - Query: {user_query}
    - Intents: {intents}
    - Metrics: {metrics}
    - Past issues to avoid: {insights}
    Generate execution plan using this tool mapping: {intent_tool_mapping}
    Use ONLY 'tools' key for tool definitions. NEVER use 'steps'â€”using 'steps' will be rejected.
    Multiple tools can be used per intent (e.g., compute_metric then generate_plot for compare_metric).
    Return valid JSON plan with EXACT structure: [{"intent": "intent_name", "tools": [{"tool": "tool_name", "args": {"metric": "metric_name", "intent_filter": ["intent_name"], "days_ahead": 30}}]}]
    Ensure the response is valid JSON and contains only the JSON object.
    Example: [{"intent": "fetch_metric", "tools": [{"tool": "fetch_metric", "args": {"metric": "efficiency", "intent_filter": ["fetch_metric"], "days_ahead": 30}}]}]
    """
    
    prompt = prompt_template.format(
        user_query=state.user_query,
        intents=intents,
        metrics=metrics,
        insights=insights,
        intent_tool_mapping=json.dumps(self.intent_tool_mapping, indent=2)
    )
    print("Generated Prompt:\n", prompt)  # Debug prompt

    # 4. Execute LLM and parse response
    try:
        response = self.llm.invoke(prompt)
        raw_response_content = response.content.strip()
        print("Raw LLM Response:\n", raw_response_content)  # Debug raw response
        cleaned_response = raw_response_content.replace("```json", "").replace("```", "").strip()
        plan = json.loads(cleaned_response)
        
        # 5. Validate and normalize plan
        validated_plan = []
        for item in plan:
            if isinstance(item, dict) and item.get("intent") in intents:
                tools = item.get("tools", [])
                if not tools:
                    tools = [{"tool": item["intent"], "args": {"metric": metrics[0] if metrics else "efficiency", "intent_filter": [item["intent"]]}}]
                validated_plan.append({"intent": item["intent"], "tools": tools})
        print("Processed Plan:", json.dumps(validated_plan, indent=2))  # Debug processed plan
        return validated_plan if validated_plan else [{"intent": intents[0], "tools": [{"tool": intents[0], "args": {"metric": metrics[0] if metrics else "efficiency", "intent_filter": [intents[0]]}}]}]
        
    except Exception as e:
        logging.error(f"Plan generation failed: {str(e)}. Raw response: {raw_response_content if 'raw_response_content' in locals() else 'N/A'}")
        # 6. Fallback plan generation
        return [{
            "intent": intent,
            "tools": [{
                "tool": next((tool for tool in self.intent_tool_mapping.get(intent, [intent]) if tool), intent),
                "args": {
                    "metric": metrics[0] if metrics else "efficiency",
                    "intent_filter": [intent]
                }
            }]
        } for intent in intents]
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
def generate_plan(self, state: AgentState, metrics: List[str]) -> List[Dict]:
    insights = self.query_agent.feedback_collector.learn_from_feedback()
    # Get intents from extracted_context if available
    intents = [ctx["intent"] for ctx in state.extracted_context.get("query_analysis", [])] if state.extracted_context else []
    prompt_template = """
Given query: {user_query}
Intents: {intents}
Context: {context}
Available metrics: {metrics}
Past feedback summary (adapt to avoid mistakes): {insights}
Plan steps/tools to execute, then call tools for each intent based on mapping {intent_tool_mapping}.
Use ONLY 'tools' key for tool definitions. NEVER use 'steps'---using 'steps' will result in an invalid response.
Multiple tools can be used per intent (e.g., compute_metric then generate_plot for compare_metric).
Return valid JSON plan with EXACT structure: [{"intent": "intent_name", "tools": [{"tool": "tool_name", "args": {"metric": "metric_name", "intent_filter": ["intent_name"], "days_ahead": 30}}]}]
Ensure the response is valid JSON and contains only the JSON object.
Example: [{"intent": "fetch_metric", "tools": [{"tool": "fetch_metric", "args": {"metric": "efficiency", "intent_filter": ["fetch_metric"], "days_ahead": 30}}]}]
"""
    prompt = prompt_template.format(
        user_query=state.user_query,
        intents=intents,  # Use the derived intents list
        context=state.extracted_context,
        metrics=metrics,
        insights=insights,
        intent_tool_mapping=json.dumps(self.intent_tool_mapping)
    )
    print("Generated Prompt:\n", prompt)
    response = self.llm.invoke(prompt)
    raw_response_content = response.content.strip()
    print("Raw LLM Response:\n", raw_response_content)
    try:
        cleaned_response = raw_response_content.replace("```json", "").replace("```", "").strip()
        plan = json.loads(cleaned_response)
        # Convert "steps" to "tools" before returning
        for i, step in enumerate(plan):
            if "steps" in step and "tools" not in step:
                plan[i]["tools"] = plan[i].pop("steps")
            elif "tools" not in step:
                plan[i]["tools"] = [{"tool": step.get("intent", ""), "args": {"metric": metrics[0] if metrics else "efficiency", "intent_filter": [step.get("intent", "")]}}]
        print("Processed Plan:", json.dumps(plan, indent=2))  # Debug the processed plan
        return plan
    except json.JSONDecodeError as e:
        logging.warning(f"JSON parse error: {str(e)}. Raw response: {raw_response_content}. Returning default plan.")
        default_intents = [ctx["intent"] for ctx in state.extracted_context.get("query_analysis", [])] if state.extracted_context else []
        return [{"intent": intent, "tools": [{"tool": intent, "args": {"metric": metrics[0] if metrics else "efficiency", "intent_filter": [intent]}}]} for intent in default_intents]

def process(self, state: AgentState) -> AgentState:
    state.extracted_context = self.query_agent.process(state.user_query)
    if state.extracted_context is not None:
        print("QueryUnderstandingAgent Output:")
        print(json.dumps(state.extracted_context, indent=2))
    
    state.data = self.data_agent.fetch_data(state.extracted_context)
    if state.data is not None and not state.data.empty:
        print("DataRetrievalAgent Final DataFrame:")
        display(state.data.head())
        print(f"Shape: {state.data.shape}")
        state.data.columns = state.data.columns.str.strip()  # Remove leading/trailing spaces
        state.data.columns = state.data.columns.str.replace(' ', '_')  # Replace inner spaces with underscores
        state.data.columns = state.data.columns.str.lower()  # Convert to lowercase for consistency
        print("Normalized DataFrame Column Names:")
        print(state.data.columns.tolist())
    else:
        print("No data available after retrieval - skipping normalization.")
    
    if state.data is None:
        state.errors = ["Data retrieval failed"]
        return state
    all_metrics_to_compute = set()
    for intent_ctx in state.extracted_context.get("query_analysis", []):
        all_metrics_to_compute.update(intent_ctx.get("metrics", []))
    if all_metrics_to_compute:
        state.data = self.analysis_agent.compute_metrics(state.data, list(all_metrics_to_compute))
        logging.info(f"Metrics computed. DataFrame columns: {state.data.columns.tolist()}")
    else:
        logging.warning("No metrics identified for computation.")
    plan = self.generate_plan(state, list(all_metrics_to_compute))
    print("Generated Plan:", json.dumps(plan, indent=2))  # Debug the plan
    analysis_results = {}
    for step in plan:
        intent = step["intent"]
        tools = step.get("tools", [])  # Use .get() to avoid KeyError
        if not tools:  # Fallback if no tools defined
            tools = [{"tool": intent, "args": {"metric": next(iter(all_metrics_to_compute)) if all_metrics_to_compute else "efficiency", "intent_filter": [intent]}}]
        for tool_step in tools:
            tool_name = tool_step.get("tool", intent)
            args = tool_step.get("args", {})
            args["data"] = state.data
            args["extracted_context"] = state.extracted_context
            if "metrics" not in args:
                for qa in state.extracted_context.get("query_analysis", []):
                    if qa["intent"] == intent and "metrics" in qa:
                        args["metrics"] = qa["metrics"]
                        break
            result = self.execute_tool(tool_name, args)
            if "error" not in result:
                if intent not in analysis_results:
                    analysis_results[intent] = {}
                analysis_results[intent][tool_name] = result
            else:
                if intent not in analysis_results:
                    analysis_results[intent] = {}
                analysis_results[intent][tool_name] = result
    state.analysis_results = analysis_results
    report_parts = []
    for intent, tool_results in analysis_results.items():
        report_parts.append(f"**{intent.replace('_', ' ').title()}:**")
        for tool_name, res in tool_results.items():
            report_parts.append(f"  - {tool_name.replace('_', ' ').title()}: {res.get('summary', str(res))}")
            if 'table' in res:
                report_parts.append(res['table'])
            if 'plot' in res:
                report_parts.append(f"    |Plot: {res['plot']}|")
            if 'prediction' in res:
                report_parts.append(f"    Prediction: {res['prediction']} on {res.get('date', 'N/A')}")
            if 'report' in res:
                report_parts.append(res['report'])
            if 'error' in res:
                report_parts.append(f"    Error: {res['error']}")
    
    state.report = "\n".join(report_parts) if report_parts else "No analysis results to report."
    programmatic_feedback = "Processed successfully" if not any("error" in intent_res.values() for intent_res in analysis_results.values()) else "Error occurred during processing or analysis"
    self.query_agent.feedback_collector.add_feedback(state.user_query, programmatic_feedback, "programmatic")
    user_feedback = self.query_agent.feedback_collector.collect_user_feedback(state.user_query)
    if user_feedback and any(word in user_feedback.lower() for word in ["error", "wrong", "inaccurate", "missing"]):
        self.query_agent.vector_store.add_query(f"Bad example: {state.user_query} - Feedback: {user_feedback}")
    
    insights = self.query_agent.feedback_collector.get_feedback_insights()
    if insights:
        state.report += "\n\n**Feedback Insights:** " + json.dumps(insights)
    
    return state

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

def compute_metrics(self, df: pd.DataFrame, metrics: List[str], intent_filter: List[str] = None) -> pd.DataFrame:
    if intent_filter and df.empty:
        logging.warning(f"No data for intent_filter: {intent_filter}")
        return df
    if intent_filter:
        df = df[df['intent'].apply(lambda x: any(i in x for i in intent_filter))]
    col_map = {col.strip(): col for col in df.columns}  # Preserve original case, strip spaces
    temp_cols = []
    for metric in metrics:
        if metric not in self.semantic_layer['metrics']:
            continue
        formula = self.semantic_layer['metrics'][metric]['formula']
        parts = formula.split('/')
        num_expr = parts[0].replace('SUM(', '').replace(')', '').strip().replace(' ', '')  # Remove all spaces
        den_expr = parts[1].replace('SUM(', '').replace(')', '').strip().replace(' ', '') if len(parts) > 1 else '1'
        
        # Debug print to inspect exact expr values
        print(f"Debug: For metric '{metric}', num_expr = '{num_expr}', den_expr = '{den_expr}'")
        
        if ',' in num_expr:
            num_cols = [col_map.get(col.strip(), col.strip()) for col in num_expr.split(',')]
            df['num_temp'] = df[num_cols].sum(axis=1, skipna=True)
            num_expr = 'num_temp'
            temp_cols.append('num_temp')
        if ',' in den_expr:
            den_cols = [col_map.get(col.strip(), col.strip()) for col in den_expr.split(',')]
            df['den_temp'] = df[den_cols].sum(axis=1, skipna=True)
            den_expr = 'den_temp'
            temp_cols.append('den_temp')
        groupby_dims = self.semantic_layer['metrics'][metric].get('dimensions', ['country_code', 'drive_month'])[:2]
        if df.empty:
            logging.warning(f"No data for grouping in metric {metric}")
            continue
        grouped = df.groupby(groupby_dims).agg({num_expr: 'sum', den_expr: 'sum'}).reset_index()
        if grouped.empty:
            logging.warning(f"Empty grouped DataFrame for metric {metric}")
            continue
        # Apply constraints
        if 'constraints' in self.semantic_layer['metrics'][metric]:
            for constraint in self.semantic_layer['metrics'][metric]['constraints']:
                if 'Total_KM > 0' in constraint and grouped[den_expr].eq(0).all():
                    grouped[metric] = np.nan
        grouped[metric] = grouped[num_expr] / grouped[den_expr].replace(0, np.nan)
        # Apply business rules
        if metric == 'efficiency' and 'business_rules' in self.semantic_layer['metrics'][metric]:
            mask = (grouped[metric] < 0) | (grouped[metric] > 2)
            grouped.loc[mask, metric] = np.nan
        df = df.merge(grouped[[*groupby_dims, metric]], on=groupby_dims, how='left')  # Fixed: Use grouped[[*groupby_dims, metric]]
    df.drop(columns=temp_cols, inplace=True, errors='ignore')

    return df

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

def generate_plan(self, state: AgentState, metrics: List[str]) -> List[Dict]:
    insights = self.query_agent.feedback_collector.learn_from_feedback()
    prompt_template = """
    Given query: {user_query}
    Intents: {intents}
    Context: {context}
    Available metrics: {metrics}
    Past feedback summary (adapt to avoid mistakes): {insights}
    Plan steps/tools to execute, then call tools for each intent based on mapping {intent_tool_mapping}.
    Use ONLY 'tools' key for tool definitions. NEVER use 'steps'â€”using 'steps' will result in an invalid response. 
    Multiple tools can be used per intent (e.g., compute_metric then generate_plot for compare_metric).
    Return valid JSON plan with EXACT structure: [{"intent": "intent_name", "tools": [{"tool": "tool_name", "args": {"metric": "metric_name", "intent_filter": ["intent_name"], "days_ahead": 30}}]}]
    Ensure the response is valid JSON and contains only the JSON object.
    Example: [{"intent": "fetch_metric", "tools": [{"tool": "fetch_metric", "args": {"metric": "efficiency", "intent_filter": ["fetch_metric"], "days_ahead": 30}}]}]
    """
    
    prompt = prompt_template.format(
        user_query=state.user_query,
        intents=state.intent,
        context=state.extracted_context,
        metrics=metrics,
        insights=insights,
        intent_tool_mapping=json.dumps(self.intent_tool_mapping)
    )
    
    print("Generated Prompt:\n", prompt)
    response = self.llm.invoke(prompt)
    raw_response_content = response.content.strip()
    print("Raw LLM Response:\n", raw_response_content)
    try:
        cleaned_response = raw_response_content.replace("```json", "").replace("```", "").strip()
        plan = json.loads(cleaned_response)
        # Convert "steps" to "tools" before returning
        for i, step in enumerate(plan):
            if "steps" in step and "tools" not in step:
                plan[i]["tools"] = plan[i].pop("steps")
            elif "tools" not in step:
                plan[i]["tools"] = [{"tool": step.get("intent", ""), "args": {"metric": metrics[0] if metrics else "efficiency", "intent_filter": [step.get("intent", "")]}}]
        print("Processed Plan:", json.dumps(plan, indent=2))  # Debug the processed plan
        return plan
    except json.JSONDecodeError as e:
        logging.warning(f"JSON parse error: {str(e)}. Raw response: {raw_response_content}. Returning default plan.")
        default_intents = [ctx["intent"] for ctx in state.extracted_context.get("query_analysis", [])]
        return [{"intent": intent, "tools": [{"tool": intent, "args": {"metric": metrics[0] if metrics else "efficiency", "intent_filter": [intent]}}]} for intent in default_intents]



